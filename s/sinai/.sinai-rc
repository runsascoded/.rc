#!/usr/bin/env bash

# Sinai path vars and aliases; source me from .bashrc!

export sinai="$s/sinai"

if [ -d "$HOME/data" ]; then
  export data="$HOME/data"
else
  echo "Warning: couldn't find \$data directory at $sinai/data or $HOME/data" 1>&2
fi

export DATA="$data"
export refs="$data"/refs
export chr20="$data/training/chr20"
export set3="$data/set3"
export set4="$data/set4"

export ints="$c/internal-tools"
export INTERNAL_TOOLS="$ints"

export YARN_HELPERS_DROP_HOST_SUFFIX_FROM=".demeter.hpc.mssm.edu_"
export YARN_LOGS_USER=hdfs
export ylh="$c/yarn-logs-helpers"
export yarn_aliases_dir="$s"/yarn
try_source "$ylh/.yarn-logs-helpers.sourceme"

try_source "$ints/environment/paths.sourceme"

export its="$ints/scripts"

# Sinai Hadoop/Demeter paths
export huser="willir31"
export hhome="/user/$huser"
export hh="$hhome"
export hdfs="hdfs://demeter-nn1.demeter.hpc.mssm.edu"
export Hhome="${hdfs}${hhome}"
export Hh="${hdfs}${hh}"
export HH="$Hh"
export hn="/hdfs/user/willir31"

export hdata="${hh}/data"
export Hdata="${hdfs}${hdata}"
export hrefs="$hdata/refs"

export hset3="$hdata/set3"
export h3="$hset3"
export h3n="$hset3/normal"
export h3t="$hset3/tumor"

export hset4="$hdata/set4"
export h4="$hset4"
export h4n="$hset4/normal"
export h4t="$hset4/tumor"

export h100k="${hdata}/100k"
export H100k="${hdfs}${h100k}"

export hout="${h100k}/out"
export Hout="${hdfs}${hout}"

export dream="/datasets/dream/data"
export training="$dream/training"
export real="$dream/real-data-sorted"

export panc="$real/pancreatic"
export panc0="$panc/PCSI0023"
export panc1="$panc/PCSI0044"
export panc2="$panc/PCSI0046"
export panc3="$panc/PCSI0048"
export panc4="$panc/PCSI0072"

export pros="$real/prostate"
export pros0="$pros/CPCG0100"
export pros1="$pros/CPCG0183"
export pros2="$pros/CPCG0184"
export pros3="$pros/CPCG0196"
export pros4="$pros/CPCG0235"

export hsyn3="$dream/synthetic-challenge-3"
export hsyn4="$dream/synthetic-challenge-4"

# Spark/ADAM opts
if [[ `uname -a` =~ ^Darwin ]]; then
  export sparklogs="$HOME/sparklogs"
  export sl="$sparklogs"
  export SPARK_LOG_DIR="$sl"
else
  #export sl="/spark/tmp/logs"
  export sl="/user/spark/applicationHistory"
  export sel="/user/spark/applicationHistory"
  export sparklogs="${hdfs}${sl}"
  export hsl="$sparklogs"
fi
export SPARK_EVENTLOG_DIR="$sparklogs"

export em="--executor-memory"
export ec="--executor-cores"
export ne="--num-executors"
export dm="--driver-memory"
export mf="--conf spark.memory.fraction"
export smf="--conf spark.memory.storageFraction"
export el="--conf spark.eventLog.enabled=true --conf spark.eventLog.dir=$sparklogs"
export mo="--conf spark.yarn.executor.memoryOverhead"
export emo="--conf spark.yarn.executor.memoryOverhead"
export dmo="--conf spark.yarn.driver.memoryOverhead"
export pll="--conf spark.default.parallelism"
export sdp="$pll"
export mef="--conf spark.yarn.max.executor.failures"
export spec="--conf spark.speculation=true --conf spark.speculation.multiplier=2 --conf spark.speculation.interval"
export dae="--conf spark.dynamicAllocation.enabled=true --conf spark.shuffle.service.enabled=true"
export mine="--conf spark.dynamicAllocation.minExecutors"
export maxe="--conf spark.dynamicAllocation.maxExecutors"
export inie="--conf spark.dynamicAllocation.initialExecutors"
export eto="--conf spark.dynamicAllocation.executorIdleTimeout"
export noui="--conf spark.ui.enabled=false"
export mxf="--conf spark.task.maxFailures"
export maxf="--conf spark.task.maxFailures"
export djo="--conf spark.driver.extraJavaOptions"
export ejo="--conf spark.executor.extraJavaOptions"
export kryo="--conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kryo.registrator"

export slim="--packages org.hammerlab:spark-json-relay:2.0.1 --conf spark.extraListeners=org.apache.spark.JsonRelay"
export slim3="$slim --conf spark.slim.host=hammerlab-dev3.hpc.mssm.edu"

if [ -z "$m2" ]; then
  echo "Error: \$m2 not set when setting Maven env vars" >&2
fi
export m2spark="$m2/org/apache/spark"
export m2sp="$m2/org/apache/spark"
export m2spc="$m2sp/spark-core_2.10"
export msc="$m2sp/spark-core_2.10"
export mac="$m2adam/adam-core_2.10"

export m2h="$m2/org/hammerlab"
export m2p="$m2h/pageant"
export m2g="$m2h/guacamole"

guac_args=
export djo="--driver-java-options"
log4j_properties=$ints/scripts/guacamole/log4j.properties
export djov="$spark_djov -Dlog4j.configuration=$log4j_properties"
guac_args="$guac_args --class org.hammerlab.guacamole.Guacamole"
guac_args="$guac_args --verbose"
guac_args="$guac_args --conf spark.shuffle.service.enabled=true"

export guac_args

export nr="--normal-reads"
export tr="--tumor-reads"
export pa="--partition-accuracy"

export SPARK_HELPERS_HADOOP_VERSION=2.6

SPARK_HISTORY_OPTS=
SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.ui.port=18082"
SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.fs.logDirectory=${SPARK_EVENTLOG_DIR}"
SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.retainedApplications=5000"
export SPARK_HISTORY_OPTS

#export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18082 -Dspark.history.fs.logDirectory=gs://hammerlab-spark/ -Dspark.history.retainedApplications=5000"

# Spark history server on gcloud
#export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18082 -Dspark.history.fs.logDirectory=gs://hammerlab-spark/ -Dspark.history.retainedApplications=5000"
#/usr/lib/spark/sbin/start-history-server.sh

export ADAM="$c/adam"
export adam="$ADAM"
export ADAM_HOME="$ADAM"
alias adam="${ADAM}/bin/adam-submit"
alias adam-local="bash ${ADAM_HOME}/adam-cli/target/appassembler/bin/adam"
alias adam-submit="${ADAM}/bin/adam-submit"
alias adam-shell="${ADAM}/bin/adam-shell"
alias a2v="adam adam2vcf"
alias v2a="adam vcf2adam"


export SPARK="$c/spark"
export spark="$SPARK"

default_spark_version="2.2.0"

if [ -z "$SPARK_HOME" ]; then
  if [ -d "$HOME/sparks/spark-${default_spark_version}-bin-hadoop2.6" ]; then
    export SPARK_HOME="$HOME/sparks/spark-${default_spark_version}-bin-hadoop2.6"
  else
    export SPARK_HOME="$SPARK"
  fi
fi

if [ -d "/etc/hadoop/conf" ]; then
  export YARN_CONF_DIR=/etc/hadoop/conf
  export HADOOP_CONF_DIR="$YARN_CONF_DIR"
fi

export PICARD="$c/picard"
export HADOOP_BAM="$c/hadoop-bam"

alias gdos="guacamole-demeter-over-ssh"

# For ADAM
export MAVEN_OPTS="-Xmx1024m -XX:MaxPermSize=256m"
export m2adam="$m2/org/bdgenomics/adam"

export arun="/hpc/users/ahujaa01"
export harun="/user/ahujaa01"

#export hg19="/hpc/users/ahujaa01/reference_genomes/hg19-reference/ucsc.hg19.fasta"
ea hg19 "$refs/hg19.fasta"
ea hhg19 "$hrefs/hg19.fasta"

export harundata="/user/ahujaa01/dream/synsets"
export tumor3="/user/ahujaa01/dream/synsets/synthetic.challenge.set3.tumor.withMDTags.chr2.bam"
export normal3="/user/ahujaa01/dream/synsets/synthetic.challenge.set3.normal.withMDTags.chr2.bam"

debug ".sinai-rc" done

export DEMETER_PORT=6000
export MINERVA_PORT=6001
export DEMETER2_PORT=6002

alias sd=". ssh-demeter"
alias sD="ssh -D $DEMETER_PORT demeter"
alias sd1="ssh demeter1"
alias sD1="ssh -D $DEMETER_PORT demeter1"
alias sd2="ssh demeter2"
alias sD2="ssh -D $DEMETER2_PORT demeter2"
alias sd3="ssh dev3"
alias sM="ssh -D $MINERVA_PORT minerva"
alias sm="ssh minerva"

alias gamrd="git add-mirror-remote demeter demeter:c/"

export DEFAULT_MIRROR_REMOTE_NAME=dev3
export DEFAULT_MIRROR_REMOTE_HOST=dev3
export DEFAULT_MIRROR_REMOTE_DIR=c
export MIRROR_REMOTES="dev,dev3,demeter,rpi"

alias gamrm="gamr minerva"
alias gcdm="gcd minerva"

export r1="/datasets/martignetti_ovarian//189/Illumina_DNA/PT189_Right_Ovary_L1/Processed/NGS.2_7_2b.WES/results/PT189_Right_Ovary_L1.PT189_Right_Ovary_L1.clean.dedup.recal.bam"
export r2="/datasets/martignetti_ovarian//189/Illumina_DNA/PT189_Right_Ovary_L2/Processed/NGS.2_7_2b.WES/results/PT189_Right_Ovary_L2.PT189_Right_Ovary_L2.clean.dedup.recal.bam"

export d1="demeter-login1.hpc.mssm.edu"
export d2="demeter-login2.hpc.mssm.edu"

export HDFS_SPARK_JAR="hdfs:///user/willir31/sparks/spark-1.6.1-bin-hadoop2.6/lib/spark-assembly-1.6.1-hadoop2.6.0.jar"
export PAGEANT_JAR="/hpc/users/willir31/c/pageant/target/pageant-with-dependencies-1.0-SNAPSHOT.jar"
#export JSON_RELAY_JAR="/hpc/users/willir31/spark-json-relay-1.0.0.jar"

export oas="org.apache.spark"
export oase="org.apache.spark.examples"
export SparkPi="org.apache.spark.examples.SparkPi"

alias act="adam-core-test"

toggle-hadoop() {
  if [ -z "$YARN_CONF_DIR" -a -z "$HADOOP_CONF_DIR" ]; then
    export YARN_CONF_DIR=/etc/hadoop/conf
    export HADOOP_CONF_DIR="$YARN_CONF_DIR"
    echo "Hadoop env enabled"
  else
    export YARN_CONF_DIR=
    export HADOOP_CONF_DIR=
    echo "Hadoop env disabled"
  fi
}

alias act=adam-core-test

export x200="/datasets/illumina_platinum/200x"
export x50="/datasets/illumina_platinum/50x"
export ps="/demeter/scratch/datasets/illumina_platinum"

export e46="$x50/ERR194146"

alias rhh=". refresh-hadoop-hosts"
alias hhs=hadoop-hosts
alias hhj=hadoop-hosts-joined

export SINAI_HADOOP_HOSTS_FILE="$HOME/.hadoop-hosts"
if [ -f "$SINAI_HADOOP_HOSTS_FILE" ]; then
  export hhs="$(cat "$SINAI_HADOOP_HOSTS_FILE")"
fi

adam-submit() {
  msp 2
  "$ADAM_HOME"/bin/adam-submit "$@"
}

defn asu adam-submit

adam-shell() {
  msp 2
  "$ADAM_HOME"/bin/adam-shell "$@"
}

defn ash adam-shell

spark-submit() {
  msp 2
  "$SPARK_HOME"/bin/spark-submit "$@"
}

defn spsu spark-submit

spark-shell() {
  msp 2
  "$SPARK_HOME"/bin/spark-shell "$@"
}

defn spsh spark-shell

esh() {
  echo "$SPARK_HOME"
}

eah() {
  echo "$ADAM_HOME"
}

ehd() {
  echo "$HADOOP_CONF_DIR"
}

eyd() {
  echo "$YARN_CONF_DIR"
}

if [ -z "$HADOOP_CONF_DIR" ]; then
  if [ -d /etc/hadoop/conf ]; then
    export HADOOP_CONF_DIR=/etc/hadoop/conf
  elif [ -d "$HOME/etc/hadoop/conf" ]; then
    export HADOOP_CONF_DIR="$HOME/etc/hadoop/conf"
  fi
fi

alias sps=spark-select
alias spb=spark-build
alias saj=spark-assembly-jar-path

alias ads=adam-select

alias dcp=demeter-cp

alias ssc=spark-shell-check

alias ashc=adam-shell-cluster
alias asuc=adam-submit-cluster

alias spshc=spark-shell-cluster
alias spsuc=spark-submit-cluster

alias asuc-small="adam-submit-cluster $dm 1g $em 5g $ec 10 $smf=0.8 $noui"

export SPARK_REPO_URL="$c/spark"

alias hpj=hadoop-put-spark-assembly-jar

export pt189="/demeter/scratch/datasets/pt189"
export p189="$pt189"
export h189="/datasets/pt189"

export d100k="$data/training/chr20/100k"
export n100k="$d100k/normal.bam"
export t100k="$d100k/tumor.bam"

export pdr="/hpc/users/rubina07/pgv-dry-run"

alias ekht=executor-killed-host-and-time

if [ -d "$gd" ]; then
  export pcf="$gd/hammerlab/tc-prostate-challenge"
fi

export dbs="$db/spark"
export a57="$dbs/application_1444948191538_0457"
export e57="$a57/events.json"
export d57="$a57/driver"

export a58="$dbs/application_1444948191538_0458"
export e58="$a58/events.json"
export d58="$a58/driver"

export a59="$dbs/application_1444948191538_0459"
export e59="$a59/events.json"
export d59="$a59/driver"

export a63="$dbs/application_1444948191538_0463"
export e63="$a63/events.json"
export d63="$a63/driver"

export a64="$dbs/application_1444948191538_0464"
export e64="$a64/events.json"
export d64="$a64/driver"

export a65="$dbs/application_1444948191538_0465"
export e65="$a65/events.json"
export d65="$a65/driver"

export AWS_HL_ACCESS="AKIAJXVAH2UO5T2M47VQ"

export ga=data/ipinivo/GA4534

export dk="$hh/data/kits"

export di="$hh/data/ipinivo"

export dr="/hpc/users/rubina07/pgv-dry-run"
export drd="$dr/DNA"
export drn="$drd/Normal"
export drt="$drd/Tumor"
export xt="$drt/SureSelect_XT"
export qxt="$drt/SureSelect_QXT"

export ssv2="$dk/sureselect-v2.bed"
export ssv4="$dk/sureselect-v4.bed"
export ssv5="$dk/sureselect-v5.bed"

export hdr="/user/willir31/pgv-dry-run"
export hdrd="$hdr/DNA"
export hdrn="$hdrd/Normal"
export hdrt="$hdrd/Tumor"
export hxt="$hdrt/SureSelect_XT"
export hqxt="$hdrt/SureSelect_QXT"

export ADAM_REPO_URL="$c/adam"

export ohg="org/hammerlab/guacamole"
export ohgd="org.hammerlab.guacamole"
export sohg="$sms/$ohg"
export tohg="$smt/$ohg"
export soh="$sms/org/hammerlab"
export toh="$sts/org/hammerlab"

export ohp="org/hammerlab/pageant"

export PICARD_JAR=$c/picard-tools-2.2.4/picard.jar
export GATK_JAR=$c/GenomeAnalysisTK-3.5/GenomeAnalysisTK.jar

export kryo="--conf spark.serializer=org.apache.spark.serializer.KryoSerializer"
export kryor="--conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kryo.registrator"
export kryorq="--conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kryo.registrationRequired=true --conf spark.kryo.registrator"
export reg="--conf spark.kryo.registrator"

export yarn="--master yarn"
export cluster="$yarn --deploy-mode cluster"
export client="$yarn --deploy-mode client"

export noretry="--conf spark.yarn.maxAppAttempts=1"

export guava="$M/com/google/guava/guava"
export guava11="$guava/11.0.2/guava-11.0.2.jar"
export guava1102="$guava/11.0.2/guava-11.0.2.jar"
export guava14="$guava/14.0.1/guava-14.0.1.jar"
export guava1401="$guava/14.0.1/guava-14.0.1.jar"
export guava16="$guava/16.0.1/guava-16.0.1.jar"
export guava1601="$guava/16.0.1/guava-16.0.1.jar"
export guava18="$guava/18.0/guava-18.0.jar"

export guac="$M/org/hammerlab/guacamole/guacamole/0.0.1-SNAPSHOT/guacamole-0.0.1-SNAPSHOT.jar"
export guacs="$M/org/hammerlab/guacamole/guacamole/0.0.1-shaded/guacamole-0.0.1-shaded.jar"

export NN="172.29.46.12"

ah() {
  cat "$s/sinai/good-hosts"
}

bh() {
  cat "$s/sinai/bad-hosts"
}

export b1="/datasets/ovarian/ega-tim/EGAZ00001018597_2014_ICGC_IcgcOvarian_AOCS034_1DNA_7PrimaryTumour_ICGCDBPC20130205009_IlluminaIGNOutsourcing_NoCapture_Bwa_HiSeq.jpearson.header_fixed.bam"
export b2="/datasets/ovarian/ega-tim/EGAZ00001018680_2014_ICGC_IcgcOvarian_AOCS034_1DNA_1NormalBlood_ICGCDBPC20130205008_IlluminaIGNOutsourcing_NoCapture_Bwa_HiSeq.jpearson.header_fixed.bam"
export b3="/datasets/ovarian/ega-tim/EGAZ00001018766_2014_ICGC_IcgcOvarian_AOCS034_1DNA_12Ascites_ICGCDBPC20130205007_IlluminaIGNOutsourcing_NoCapture_Bwa_HiSeq.jpearson.bam"

ea br src/test/resources/cancer-wgs1/recurrence.bam
ea bn src/test/resources/cancer-wgs1/normal.bam
ea bp src/test/resources/cancer-wgs1/primary.bam

ea guac_target "$HOME/c/guacamole/target"
ea guac_version 0.0.1-SNAPSHOT
ea guac_deps_jar "$guac_target/guacamole-deps-only-$guac_version.jar"
ea guac_jar "$guac_target/guacamole-$guac_version.jar"

ea guac_jars "--jars $guac_jar,$guac_deps_jar"

ea contigs "$( (seq 22; echo X) | pp chr | j, | dfl ,)"

guac_main="--class org.hammerlab.guacamole.Main --jars $guac_deps_jar $guac_jar"

ea sn "--conf spark.app.name"
ea sid "--conf spark.app.id"

ea guac_kryo "--conf spark.kryo.registrator=org.hammerlab.guacamole.kryo.Registrar"
alias guac-shell-cluster="spark-shell-cluster $guac_kryo $sn=guac-shell $guac_jars"
alias guac-shell-local="spark-shell-local $guac_kryo $sn=guac-shell $guac_jars"

#alias guac-submit-local="spark-submit-local $guac_kryo $guac_jars"

ea cn /datasets/mskcc/bladder/bams/41f31e9f603545c1bf8c7bed970707da.bam
ea ct /datasets/mskcc/bladder/bams/a77139d21e2241048b2fdcefb8c9c55c.bam

ea GUACAMOLE_HOME "$HOME/c/guacamole"
ea GUAC_HOME "$GUACAMOLE_HOME"
ea GUAC_JARS "--guacamole-jar $GUAC_HOME/target/guacamole-0.0.1-SNAPSHOT.jar --guacamole-dependencies-jar $GUAC_HOME/target/guacamole-deps-only-0.0.1-SNAPSHOT.jar"

ea GUAC2_HOME "$HOME/c/guac2"
ea GUAC2_JARS "--guacamole-jar $GUAC2_HOME/target/guacamole-0.0.1-SNAPSHOT.jar --guacamole-dependencies-jar $GUAC2_HOME/target/guacamole-deps-only-0.0.1-SNAPSHOT.jar"

#ea g target/guacamole-0.0.1-SNAPSHOT.jar
#ea gd target/guacamole-deps-0.0.1-SNAPSHOT.jar
#ea ga target/guacamole-assembly-0.0.1-SNAPSHOT.jar

ea db data/baylor
ea dbn data/baylor/TCRBOA2-N-WEX.bam
ea dbt data/baylor/TCRBOA2-T-WEX.bam

ea n2 /datasets/dream/data/synthetic-challenge-2/synthetic.challenge.set2.normal.withMDTags.bam
ea t2 /datasets/dream/data/synthetic-challenge-2/synthetic.challenge.set2.tumor.withMDTags.bam

make_cluster() {
	gcloud dataproc clusters create "${1:-pageant-test}" \
		--bucket ct-hammerlab \
		--master-machine-type n1-standard-4 \
		--worker-machine-type n1-standard-4 \
		--num-workers 2 \
		--num-preemptible-workers 50 \
		--project pici-1286
}

if [ -f "$sinai/dataproc.conf" ]; then
	export SPARK_PROPS_FILES="$sinai/dataproc.conf"
fi


export bam1="gs://hammerlab-tcga/for-msk-bms-lung/legacy_downloads/19155553-8199-4c4d-a35d-9a2f94dd2e7d/C509.TCGA-64-1681-01A-11D-2063-08.1.bam"
export bam2="gs://hammerlab-tcga/for-msk-bms-lung/legacy_downloads/b7ee4c39-1185-4301-a160-669dea90e192/C508.TCGA-85-A512-01A-11D-A26M-08.6.bam"
export hbam1="hdfs:///datasets/msk-bms-lung/C509.TCGA-64-1681-01A-11D-2063-08.1.bam"
export hbam2="hdfs:///datasets/msk-bms-lung/C508.TCGA-85-A512-01A-11D-A26M-08.6.bam"
export gcn=google-cloud-nio-0.20.0-alpha-shaded.jar
export sba=spark-bam-assembly-1.1.0-SNAPSHOT.jar
export sm=org.hammerlab.bam.spark.Main
export cm=org.hammerlab.bam.check.Main
export cmpm=org.hammerlab.bam.spark.compare.Main
export bed=gs://ct-hammerlab/misc/sorted-ensembl-exons-r75.bed
export cdj=coverage-depth-assembly-1.0.0-SNAPSHOT.jar

#export SPARK_PROPERTIES_FILES=gs://hammerlab-spark/conf/full

export out1="gs://hammerlab-tcga/for-msk-bms-lung/legacy_pageant/C509.TCGA-64-1681-01A-11D-2063-08.1.bam"
export out2="gs://hammerlab-tcga/for-msk-bms-lung/legacy_pageant/C508.TCGA-85-A512-01A-11D-A26M-08.6.bam"

cvm=org.hammerlab.coverage.Main
cvj=coverage-depth-49e6b11.jar

gtf=gs://msk-rcc/pageant/reference/Homo_sapiens.GRCh37.75.gtf

export gcn=$HOME/lib/google-cloud-nio-0.20.0-alpha-shaded.jar
