#!/usr/bin/env bash

# Sinai path vars and aliases; source me from .bashrc!

export sinai="$HOME/sinai"

if [ -d "$sinai/data" ]; then
  export data="$sinai/data"
elif [ -d "$HOME/data" ]; then
  export data="$HOME/data"
else
  err "Couldn't find \$data directory at $sinai/data or $HOME/data"
fi

export DATA="$data"
export refs="$data"/refs
export chr20="$data/training/chr20"
export set3="$data/set3"
export set4="$data/set4"

export guac="$c/guacamole"
export GUAC="$guac"

export ints="$c/internal-tools"
export INTERNAL_TOOLS="$ints"

export YARN_HELPERS_DROP_HOST_SUFFIX_FROM=".demeter.hpc.mssm.edu_"
export YARN_HELPERS_DRIVER_GREP_NEEDLE="Guacamole starting."
export ylh="$c/yarn-logs-helpers"
export yarn_aliases_dir="$s"/yarn
try_source "$ylh/.yarn-logs-helpers.sourceme"

try_source "$ints/environment/paths.sourceme"

export its="$ints/scripts"
alias mvnt="mvn-test"

# Sinai Hadoop/Demeter paths
export huser="willir31"
export hhome="/user/$huser"
export hh="$hhome"
export hdfs="hdfs://demeter-nn1.demeter.hpc.mssm.edu"
export Hhome="${hdfs}${hhome}"
export Hh="${hdfs}${hh}"
export HH="$Hh"

export hdata="${hh}/data"
export Hdata="${hdfs}${hdata}"
export hrefs="$hdata/refs"

export hset3="$hdata/set3"
export h3="$hset3"
export h3n="$hset3/normal"
export h3t="$hset3/tumor"

export hset4="$hdata/set4"
export h4="$hset4"
export h4n="$hset4/normal"
export h4t="$hset4/tumor"

export h100k="${hdata}/100k"
export H100k="${hdfs}${h100k}"

export hout="${h100k}/out"
export Hout="${hdfs}${hout}"

export dream="/datasets/dream/data"
export training="$dream/training"
export real="$dream/real-data-sorted"

export panc="$real/pancreatic"
export panc0="$panc/PCSI0023"
export panc1="$panc/PCSI0044"
export panc2="$panc/PCSI0046"
export panc3="$panc/PCSI0048"
export panc4="$panc/PCSI0072"

export pros="$real/prostate"
export pros0="$pros/CPCG0100"
export pros1="$pros/CPCG0183"
export pros2="$pros/CPCG0184"
export pros3="$pros/CPCG0196"
export pros4="$pros/CPCG0235"

export hsyn3="$dream/synthetic-challenge-3"
export hsyn4="$dream/synthetic-challenge-4"

# Spark/ADAM opts
export em="--executor-memory"
export ec="--executor-cores"
export ne="--num-executors"
export dm="--driver-memory"
export mf="--conf spark.storage.memoryFraction"
export smf="--conf spark.shuffle.memoryFraction"
export el="--conf spark.eventLog.enabled=true --conf spark.eventLog.dir=$hdfs:"
export dp="--conf spark.storage.memoryFraction"
export mo="--conf spark.yarn.executor.memoryOverhead"
export pll="--conf spark.default.parallelism"
export sdp="$pll"

if [[ `uname -a` =~ ^Darwin ]]; then
  export sparklogs="$HOME/sparklogs"
  export sl="$sparklogs"
else
  export sl="/spark/tmp/logs"
  export sparklogs="${hdfs}${sl}"
  export hsl="$sparklogs"
fi
export SPARK_EVENTLOG_DIR="$sparklogs"

if [ -z "$m2" ]; then
  echo "Error: \$m2 not set when setting Maven env vars" >&2
fi
export m2spark="$m2/org/apache/spark"
export m2core="$m2/org/apache/spark/spark-core_2.10"
export msc="$m2spark/spark-core_2.10"
export mac="$m2adam/adam-core_2.10"

guac_args=
export djo="--driver-java-options"
log4j_properties=$ints/scripts/guacamole/log4j.properties
export djov="$spark_djov -Dlog4j.configuration=$log4j_properties"
guac_args="$guac_args --class org.hammerlab.guacamole.Guacamole"
guac_args="$guac_args --verbose"
guac_args="$guac_args --conf spark.shuffle.service.enabled=true"

export guac_args

export nr="--normal-reads"
export tr="--tumor-reads"
export pa="--partition-accuracy"

export SPARK_HELPERS_HADOOP_VERSION=2.6

SPARK_HISTORY_OPTS=
SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.ui.port=18082"
SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.fs.logDirectory=${SPARK_EVENTLOG_DIR}"
SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.retainedApplications=5000"
export SPARK_HISTORY_OPTS

export ADAM="$c/adam"
export adam="$ADAM"
export ADAM_HOME="$ADAM"
alias adam="${ADAM}/bin/adam-submit"
alias adam-local="bash ${ADAM_HOME}/adam-cli/target/appassembler/bin/adam"
alias adam-submit="${ADAM}/bin/adam-submit"
alias adam-shell="${ADAM}/bin/adam-shell"
alias a2v="adam adam2vcf"
alias v2a="adam vcf2adam"


export SPARK="$c/spark"
export spark="$SPARK"
if [ -z "$SPARK_HOME" ]; then
  if [ -d "$HOME/sparks/spark-1.5.0-bin-hadoop2.6" ]; then
    export SPARK_HOME="$HOME/sparks/spark-1.5.0-bin-hadoop2.6"
  else
    export SPARK_HOME="$SPARK"
  fi
fi

if [ -d "/etc/hadoop/conf" ]; then
  export YARN_CONF_DIR=/etc/hadoop/conf
  export HADOOP_CONF_DIR="$YARN_CONF_DIR"
fi

export PICARD="$c/picard"
export HADOOP_BAM="$c/hadoop-bam"

alias gdos="guacamole-demeter-over-ssh"

# For ADAM
export "MAVEN_OPTS=-Xmx1024m -XX:MaxPermSize=256m"
export m2adam="$m2/org/bdgenomics/adam"

export arun="/hpc/users/ahujaa01"
export harun="/user/ahujaa01"

export hg19="/hpc/users/ahujaa01/reference_genomes/Homo_sapiens_assembly19.fasta"
export hhg19="$hrefs/hg19.fasta"

export harundata="/user/ahujaa01/dream/synsets"
export tumor3="/user/ahujaa01/dream/synsets/synthetic.challenge.set3.tumor.withMDTags.chr2.bam"
export normal3="/user/ahujaa01/dream/synsets/synthetic.challenge.set3.normal.withMDTags.chr2.bam"

debug ".sinai-rc" done

export DEMETER_PORT=6000
export MINERVA_PORT=6001

alias sd=". ssh-demeter"
alias sD="ssh -D $DEMETER_PORT demeter"
alias sd1="ssh demeter1"
alias sD1="ssh -D $DEMETER_PORT demeter1"
alias sd2="ssh demeter2"
alias sD2="ssh -D $DEMETER_PORT demeter2"
alias sd3="ssh dev3"
alias sM="ssh -D $MINERVA_PORT minerva"
alias sm="ssh minerva"

alias gamrd="git add-mirror-remote demeter demeter:c/"

export DEFAULT_MIRROR_REMOTE_NAME=dev3
export DEFAULT_MIRROR_REMOTE_HOST=dev3
export DEFAULT_MIRROR_REMOTE_DIR=c
export MIRROR_REMOTES="dev3,demeter,rpi"

alias gamrm="gamr minerva"
alias gcdm="gcd minerva"

export r1="/datasets/martignetti_ovarian//189/Illumina_DNA/PT189_Right_Ovary_L1/Processed/NGS.2_7_2b.WES/results/PT189_Right_Ovary_L1.PT189_Right_Ovary_L1.clean.dedup.recal.bam"
export r2="/datasets/martignetti_ovarian//189/Illumina_DNA/PT189_Right_Ovary_L2/Processed/NGS.2_7_2b.WES/results/PT189_Right_Ovary_L2.PT189_Right_Ovary_L2.clean.dedup.recal.bam"

export adam_shell_args="--master yarn --deploy-mode client $ne 10 $ec 4 $em 10g $dm 10g $mf=0.05 --conf spark.shuffle.memoryFraction=0.4 --conf spark.akka.timeout=100000 --conf spark.shuffle.service.enabled=true --conf spark.shuffle.manager=SORT $mo=1024 --conf spark.file.transferTo=false"

export d1="demeter-login1.hpc.mssm.edu"
export d2="demeter-login2.hpc.mssm.edu"

export HDFS_SPARK_JAR="hdfs:///user/willir31/sparks/spark-1.5.0-bin-hadoop2.6/lib/spark-assembly-1.5.0-hadoop2.6.0.jar"
export PAGEANT_JAR="/hpc/users/willir31/c/pageant/target/pageant-with-dependencies-1.0-SNAPSHOT.jar"
#export JSON_RELAY_JAR="/hpc/users/willir31/spark-json-relay-1.0.0.jar"

export oas="org.apache.spark"
export oase="org.apache.spark.examples"
export SparkPi="org.apache.spark.examples.SparkPi"

alias act="adam-core-test"

toggle-hadoop() {
  if [ -z "$YARN_CONF_DIR" -a -z "$HADOOP_CONF_DIR" ]; then
    export YARN_CONF_DIR=/etc/hadoop/conf
    export HADOOP_CONF_DIR="$YARN_CONF_DIR"
    echo "Hadoop env enabled"
  else
    export YARN_CONF_DIR=
    export HADOOP_CONF_DIR=
    echo "Hadoop env disabled"
  fi
}

alias act=adam-core-test

export x200="/datasets/illumina_platinum/200x"
export x50="/datasets/illumina_platinum/50x"
export ps="/demeter/scratch/datasets/illumina_platinum"

alias rhh=". refresh-hadoop-hosts"
alias hhs=hadoop-hosts
alias hhj=hadoop-hosts-joined

export SINAI_HADOOP_HOSTS_FILE="$HOME/.hadoop-hosts"
if [ -f "$SINAI_HADOOP_HOSTS_FILE" ]; then
  export hhs="$(cat "$SINAI_HADOOP_HOSTS_FILE")"
fi

asu() {
  "$ADAM_HOME"/bin/adam-submit "$@"
}

ash() {
  "$ADAM_HOME"/bin/adam-shell "$@"
}

spsu() {
  "$SPARK_HOME"/bin/spark-submit "$@"
}

spsh() {
  "$SPARK_HOME"/bin/spark-shell "$@"
}

esh() {
  echo "$SPARK_HOME"
}

eah() {
  echo "$ADAM_HOME"
}

ehd() {
  echo "$HADOOP_CONF_DIR"
}

eyd() {
  echo "$YARN_CONF_DIR"
}

alias sps=spark-select
alias spb=spark-build
alias ads=adam-select

