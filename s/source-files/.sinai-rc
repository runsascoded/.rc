#!/bin/bash

# Sinai path vars and aliases; source me from .bashrc!

export sinai="$HOME/sinai"

if [ -d "$sinai/data" ]; then
  export data="$sinai/data"
elif [ -d "$HOME/data" ]; then
  export data="$HOME/data"
else
  err "Couldn't find \$data directory at $sinai/data or $HOME/data"
fi

export DATA="$data"
export refs="$data"/refs
export chr20="$data/training/chr20"
export set3="$data/set3"
export set4="$data/set4"

export guac="$sinai/guacamole"
export GUAC="$guac"

export ints="$sinai/internal-tools"
export INTERNAL_TOOLS="$ints"

export YARN_HELPERS_DROP_HOST_SUFFIX_FROM=".demeter.hpc.mssm.edu_"
export YARN_HELPERS_DRIVER_GREP_NEEDLE="Guacamole starting."
export ylh="$c/yarn-logs-helpers"
export yarn_aliases_dir="$s"/yarn
try_source "$ylh/.yarn-logs-helpers.sourceme"

try_source "$ints/environment/paths.sourceme"

export its="$ints/scripts"
alias mvnt="mvn-test"

alias bak-iml="cp $guac/guacamole.iml ~/Dropbox/guacamole.iml.$(dt)"

# Sinai Hadoop/Demeter paths
export huser="willir31"
export hhome="/user/$huser"
export hh="$hhome"
export hdfs="hdfs://demeter-nn1.demeter.hpc.mssm.edu"
export Hhome="${hdfs}${hhome}"
export Hh="${hdfs}${hh}"
export HH="$Hh"

export hdata="${hh}/data"
export Hdata="${hdfs}${hdata}"
export hrefs="$hdata/refs"

export hset3="$hdata/set3"
export h3="$hset3"
export h3n="$hset3/normal"
export h3t="$hset3/tumor"

export hset4="$hdata/set4"
export h4="$hset4"
export h4n="$hset4/normal"
export h4t="$hset4/tumor"

export h100k="${hdata}/100k"
export H100k="${hdfs}${h100k}"

export hout="${h100k}/out"
export Hout="${hdfs}${hout}"

export dream="/datasets/dream/data"
export training="$dream/training"
export real="$dream/real-data-sorted"

export panc="$real/pancreatic"
export panc0="$panc/PCSI0023"
export panc1="$panc/PCSI0044"
export panc2="$panc/PCSI0046"
export panc3="$panc/PCSI0048"
export panc4="$panc/PCSI0072"

export pros="$real/prostate"
export pros0="$pros/CPCG0100"
export pros1="$pros/CPCG0183"
export pros2="$pros/CPCG0184"
export pros3="$pros/CPCG0196"
export pros4="$pros/CPCG0235"

export hsyn3="$dream/synthetic-challenge-3"
export hsyn4="$dream/synthetic-challenge-4"

# Spark/ADAM opts
export em=--executor-memory
export ec=--executor-cores
export ne=--num-executors
export dm=--driver-memory
export mf="--conf spark.storage.memoryFraction"
export el="--conf spark.eventLog.enabled=true --conf spark.eventLog.dir=$hdfs:"
export dp="--conf spark.storage.memoryFraction"
export mo="--conf spark.yarn.executor.memoryOverhead"
export pll="--conf spark.default.parallelism"

export oldsparklogs="/spark/tmp/logs"
export sparklogs="/spark/tmp/logs/$huser"
export hsparklogs="${hdfs}${sparklogs}"
export mysparklogs="$hhome/spark.logs"

export SPARK_HADOOP_VERSION=2.3.0

spark_args=
spark_args="$spark_args --conf spark.eventLog.enabled=true"
spark_args="$spark_args --conf spark.eventLog.dir=$hdfs:$sparklogs"
spark_args="$spark_args --conf spark.default.parallelism=1000"
spark_args="$spark_args --conf spark.storage.memoryFraction=0.1"
spark_args="$spark_args --executor-memory 10g"
spark_args="$spark_args --executor-cores 4"
spark_args="$spark_args --driver-memory 10g"
spark_args="$spark_args --num-executors 1000"
spark_args="$spark_args --master yarn"
spark_args="$spark_args --deploy-mode cluster"
spark_djov="-Dyarn.resourcemanager.am.max-attempts=1"
spark_args="$spark_args --driver-java-options $spark_djov"

sort_shuffle="--conf spark.shuffle.manager=SORT"
hash_shuffle="--conf spark.shuffle.manager=HASH"

spark_args="$spark_args --conf spark.yarn.executor.memoryOverhead=1024"
spark_args="$spark_args --conf spark.app.name=Guacamole"
spark_args="$spark_args --conf spark.core.connection.ack.wait.timeout=600"
spark_args="$spark_args --conf spark.file.transferTo=false"
spark_args="$spark_args --conf spark.akka.timeout=1000"
spark_args="$spark_args --conf -Dyarn.nodemanager.aux-services=org.apache.spark.network.yarn.YarnShuffleService"
spark_args="$spark_args $sort_shuffle"
export gc_opts="-XX:+PrintGCDetails -XX:+PrintGCTimeStamps"
export ejo="spark.executor.extraJavaOptions=$gc_opts"

export spark_args

guac_args="$spark_args"
export djo="--driver-java-options"
log4j_properties=$ints/scripts/guacamole/log4j.properties
export djov="$spark_djov -Dlog4j.configuration=$log4j_properties"
guac_args="$guac_args --class org.hammerlab.guacamole.Guacamole"
guac_args="$guac_args --verbose"
export guac_args

SPARK_HISTORY_OPTS=
SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.ui.port=18081"
SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.fs.logDirectory=${hsparklogs}"
SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.retainedApplications=1000"
export SPARK_HISTORY_OPTS

export ADAM="$c/adam"
export adam="$ADAM"
export ADAM_HOME="$ADAM"
alias adam="${ADAM}/bin/adam-submit"
alias adam-local="bash ${ADAM_HOME}/adam-cli/target/appassembler/bin/adam"
alias adam-submit="${ADAM}/bin/adam-submit"
alias adam-shell="${ADAM}/bin/adam-shell"
alias a2v="adam adam2vcf"
alias v2a="adam vcf2adam"


export SPARK="$c/spark"
export spark="$SPARK"
if [ -z "$SPARK_HOME" ]; then
  export SPARK_HOME="$SPARK"
fi

if [ -d "/etc/hadoop/conf" ]; then
  export YARN_CONF_DIR=/etc/hadoop/conf
fi

export PICARD="$c/picard"
export HADOOP_BAM="$c/hadoop-bam"

alias gdos="guacamole-demeter-over-ssh"

# For ADAM
export "MAVEN_OPTS=-Xmx512m -XX:MaxPermSize=256m"
export m2adam="$m2/org/bdgenomics/adam"

export arun="/hpc/users/ahujaa01"
export harun="/user/ahujaa01"

export hg19="/hpc/users/ahujaa01/reference_genomes/Homo_sapiens_assembly19.fasta"
export hhg19="$hrefs/hg19.fasta"

export harundata="/user/ahujaa01/dream/synsets"
export tumor3="/user/ahujaa01/dream/synsets/synthetic.challenge.set3.tumor.withMDTags.chr2.bam"
export normal3="/user/ahujaa01/dream/synsets/synthetic.challenge.set3.normal.withMDTags.chr2.bam"

debug ".sinai-rc" done
