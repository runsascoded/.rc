#!/bin/bash

# Sinai path vars and aliases; source me from .bashrc!

export sinai="$HOME/sinai"

if [ -d "$sinai/data" ]; then
  export data="$sinai/data"
elif [ -d "$HOME/data" ]; then
  export data="$HOME/data"
else
  err "Couldn't find \$data directory at $sinai/data or $HOME/data"
fi

export DATA="$data"
export refs="$data"/refs
export chr20="$data/training/chr20"
export set3="$data/set3"
export set4="$data/set4"

export guac="$sinai/guacamole"
export GUAC="$guac"

export ints="$sinai/internal-tools"
export INTERNAL_TOOLS="$ints"

export yarn_appid_base="1415745397000"

try_source "$ints/environment/paths.sourceme"

export its="$ints/scripts"
alias mvnt="mvn-test"

alias bak-iml="cp $guac/guacamole.iml ~/Dropbox/guacamole.iml.$(dt)"

# Sinai Hadoop/Demeter paths
export hhome="/user/willir31"
export hh="$hhome"
export hdfs="hdfs://demeter-nn1.demeter.hpc.mssm.edu"
export Hhome="${hdfs}${hhome}"
export Hh="${hdfs}${hh}"
export HH="$Hh"

export hdata="${hh}/data"
export Hdata="${hdfs}${hdata}"

export hset3="$hdata/set3"
export h3="$hset3"
export h3n="$hset3/normal"
export h3t="$hset3/tumor"

export hset4="$hdata/set4"
export h4="$hset4"
export h4n="$hset4/normal"
export h4t="$hset4/tumor"

export h100k="${hdata}/100k"
export H100k="${hdfs}${h100k}"

export hout="${h100k}/out"
export Hout="${hdfs}${hout}"

export dream="/datasets/dream/data"
export training="$dream/training"
export real="$dream/real-data-sorted"

export panc="$real/pancreatic"
export panc0="$panc/PCSI0023"
export panc1="$panc/PCSI0044"
export panc2="$panc/PCSI0046"
export panc3="$panc/PCSI0048"
export panc4="$panc/PCSI0072"

export pros="$real/prostate"
export pros0="$pros/CPCG0100"
export pros1="$pros/CPCG0183"
export pros2="$pros/CPCG0184"
export pros3="$pros/CPCG0196"
export pros4="$pros/CPCG0235"

export hsyn3="$dream/synthetic-challenge-3"
export hsyn4="$dream/synthetic-challenge-4"

export SPARK_JAVA_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps"
export adam_args="--conf spark.eventLog.enabled=true --conf spark.eventLog.dir=hdfs://demeter-nn1.demeter.hpc.mssm.edu:/spark/tmp/logs --conf spark.default.parallelism=2000 --conf spark.storage.memoryFraction=0.1 --master yarn"

export ADAM="$c/adam"
export adam="$ADAM"
export ADAM_HOME="$ADAM"
alias adam="${ADAM}/bin/adam-submit"
alias adam-local="bash ${ADAM_HOME}/adam-cli/target/appassembler/bin/adam"
alias adam-submit="${ADAM}/bin/adam-submit"
alias adam-shell="${ADAM}/bin/adam-shell"
alias a2v="adam adam2vcf"
alias v2a="adam vcf2adam"


export SPARK="$c/spark"
export spark="$SPARK"
if [ -z "$SPARK_HOME" ]; then
  export SPARK_HOME="$SPARK"
fi
export sparklogs="hdfs://demeter-nn1.demeter.hpc.mssm.edu:/spark/tmp/logs"
export mysparklogs="$hhome/spark.logs"

if [ -d "/etc/hadoop/conf" ]; then
  export YARN_CONF_DIR=/etc/hadoop/conf
fi

export PICARD="$c/picard"
export HADOOP_BAM="$c/hadoop-bam"

alias gdos="guacamole-demeter-over-ssh"

# For ADAM
export "MAVEN_OPTS=-Xmx512m -XX:MaxPermSize=256m"
export m2adam="$m2/org/bdgenomics/adam"

export arun="/hpc/users/ahujaa01"
export harun="/user/ahujaa01"

export hg19="/hpc/users/ahujaa01/reference_genomes/Homo_sapiens_assembly19.fasta"
export hhg19="/user/willir31/data/refs/hg19.fasta"

export harundata="/user/ahujaa01/dream/synsets"
export tumor3="/user/ahujaa01/dream/synsets/synthetic.challenge.set3.tumor.withMDTags.chr2.bam"
export normal3="/user/ahujaa01/dream/synsets/synthetic.challenge.set3.normal.withMDTags.chr2.bam"
