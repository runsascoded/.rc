#!/bin/bash

# Sinai path vars and aliases; source me from .bashrc!

export sinai="$HOME/sinai"

if [ -d "$sinai/data" ]; then
  export data="$sinai/data"
elif [ -d "$HOME/data" ]; then
  export data="$HOME/data"
else
  err "Couldn't find \$data directory at $sinai/data or $HOME/data"
fi

export DATA="$data"
export refs="$data"/refs
export chr20="$data/training/chr20"
export set3="$data/set3"
export set4="$data/set4"

export guac="$c/guacamole"
export GUAC="$guac"

export ints="$c/internal-tools"
export INTERNAL_TOOLS="$ints"

export YARN_HELPERS_DROP_HOST_SUFFIX_FROM=".demeter.hpc.mssm.edu_"
export YARN_HELPERS_DRIVER_GREP_NEEDLE="Guacamole starting."
export ylh="$c/yarn-logs-helpers"
export yarn_aliases_dir="$s"/yarn
try_source "$ylh/.yarn-logs-helpers.sourceme"

try_source "$ints/environment/paths.sourceme"

export its="$ints/scripts"
alias mvnt="mvn-test"

# Sinai Hadoop/Demeter paths
export huser="willir31"
export hhome="/user/$huser"
export hh="$hhome"
export hdfs="hdfs://demeter-nn1.demeter.hpc.mssm.edu"
export Hhome="${hdfs}${hhome}"
export Hh="${hdfs}${hh}"
export HH="$Hh"

export hdata="${hh}/data"
export Hdata="${hdfs}${hdata}"
export hrefs="$hdata/refs"

export hset3="$hdata/set3"
export h3="$hset3"
export h3n="$hset3/normal"
export h3t="$hset3/tumor"

export hset4="$hdata/set4"
export h4="$hset4"
export h4n="$hset4/normal"
export h4t="$hset4/tumor"

export h100k="${hdata}/100k"
export H100k="${hdfs}${h100k}"

export hout="${h100k}/out"
export Hout="${hdfs}${hout}"

export dream="/datasets/dream/data"
export training="$dream/training"
export real="$dream/real-data-sorted"

export panc="$real/pancreatic"
export panc0="$panc/PCSI0023"
export panc1="$panc/PCSI0044"
export panc2="$panc/PCSI0046"
export panc3="$panc/PCSI0048"
export panc4="$panc/PCSI0072"

export pros="$real/prostate"
export pros0="$pros/CPCG0100"
export pros1="$pros/CPCG0183"
export pros2="$pros/CPCG0184"
export pros3="$pros/CPCG0196"
export pros4="$pros/CPCG0235"

export hsyn3="$dream/synthetic-challenge-3"
export hsyn4="$dream/synthetic-challenge-4"

# Spark/ADAM opts
export em="--executor-memory"
export ec="--executor-cores"
export ne="--num-executors"
export dm="--driver-memory"
export mf="--conf spark.storage.memoryFraction"
export smf="--conf spark.shuffle.memoryFraction"
export el="--conf spark.eventLog.enabled=true --conf spark.eventLog.dir=$hdfs:"
export dp="--conf spark.storage.memoryFraction"
export mo="--conf spark.yarn.executor.memoryOverhead"
export pll="--conf spark.default.parallelism"

export oldsparklogs="/spark/tmp/logs"
export sparklogs="/spark/tmp/logs/$huser"
export hsparklogs="${hdfs}${sparklogs}"
export mysparklogs="$hhome/spark.logs"

spark_args=
spark_args="$spark_args --conf spark.eventLog.enabled=true"
spark_args="$spark_args --conf spark.eventLog.dir=$hdfs:$sparklogs"
spark_args="$spark_args --conf spark.default.parallelism=1000"
spark_args="$spark_args --conf spark.storage.memoryFraction=0.1"
spark_args="$spark_args --executor-memory 10g"
spark_args="$spark_args --executor-cores 4"
spark_args="$spark_args --driver-memory 10g"
spark_args="$spark_args --num-executors 1000"
spark_args="$spark_args --master yarn"
spark_args="$spark_args --deploy-mode cluster"
spark_djov="-Dyarn.resourcemanager.am.max-attempts=1"
spark_args="$spark_args --driver-java-options $spark_djov"

sort_shuffle="--conf spark.shuffle.manager=SORT"
hash_shuffle="--conf spark.shuffle.manager=HASH"

spark_args="$spark_args --conf spark.yarn.executor.memoryOverhead=3072"
spark_args="$spark_args --conf spark.core.connection.ack.wait.timeout=600"
spark_args="$spark_args --conf spark.file.transferTo=false"
spark_args="$spark_args --conf spark.akka.timeout=1000"
spark_args="$spark_args --conf spark.metrics.conf=metrics.properties"
spark_args="$spark_args --files=$HOME/metrics.properties"
spark_args="$spark_args $sort_shuffle"
export gc_opts="-XX:+PrintGCDetails -XX:+PrintGCTimeStamps"
export ejo="spark.executor.extraJavaOptions=$gc_opts"

export spark_args

if [ -z "$m2" ]; then
  echo "Error: \$m2 not set when setting Maven env vars" >&2
fi
export m2spark="$m2/org/apache/spark"
export m2core="$m2/org/apache/spark/spark-core_2.10"
export msc="$m2spark/spark-core_2.10"
export mac="$m2adam/adam-core_2.10"

guac_args="$spark_args"
export djo="--driver-java-options"
log4j_properties=$ints/scripts/guacamole/log4j.properties
export djov="$spark_djov -Dlog4j.configuration=$log4j_properties"
guac_args="$guac_args --class org.hammerlab.guacamole.Guacamole"
guac_args="$guac_args --verbose"
guac_args="$guac_args --conf spark.shuffle.service.enabled=true"

export guac_args

export nr="--normal-reads"
export tr="--tumor-reads"
export pa="--partition-accuracy"


SPARK_HISTORY_OPTS=
SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.ui.port=18082"
SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.fs.logDirectory=${hsparklogs}"
SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS -Dspark.history.retainedApplications=1000"
export SPARK_HISTORY_OPTS

export ADAM="$c/adam"
export adam="$ADAM"
export ADAM_HOME="$ADAM"
alias adam="${ADAM}/bin/adam-submit"
alias adam-local="bash ${ADAM_HOME}/adam-cli/target/appassembler/bin/adam"
alias adam-submit="${ADAM}/bin/adam-submit"
alias adam-shell="${ADAM}/bin/adam-shell"
alias a2v="adam adam2vcf"
alias v2a="adam vcf2adam"


export SPARK="$c/spark"
export spark="$SPARK"
if [ -z "$SPARK_HOME" ]; then
  export SPARK_HOME="$SPARK"
fi

if [ -d "/etc/hadoop/conf" ]; then
  export YARN_CONF_DIR=/etc/hadoop/conf
fi

export PICARD="$c/picard"
export HADOOP_BAM="$c/hadoop-bam"

alias gdos="guacamole-demeter-over-ssh"

# For ADAM
export "MAVEN_OPTS=-Xmx512m -XX:MaxPermSize=256m"
export m2adam="$m2/org/bdgenomics/adam"

export arun="/hpc/users/ahujaa01"
export harun="/user/ahujaa01"

export hg19="/hpc/users/ahujaa01/reference_genomes/Homo_sapiens_assembly19.fasta"
export hhg19="$hrefs/hg19.fasta"

export harundata="/user/ahujaa01/dream/synsets"
export tumor3="/user/ahujaa01/dream/synsets/synthetic.challenge.set3.tumor.withMDTags.chr2.bam"
export normal3="/user/ahujaa01/dream/synsets/synthetic.challenge.set3.normal.withMDTags.chr2.bam"

debug ".sinai-rc" done

export DEMETER_PORT=6000
export MINERVA_PORT=6001

alias sd=". ssh-demeter"
alias sD="ssh -D $DEMETER_PORT demeter"
alias sd1="ssh demeter1"
alias sD1="ssh -D $DEMETER_PORT demeter1"
alias sd2="ssh demeter2"
alias sD2="ssh -D $DEMETER_PORT demeter2"
alias sM="ssh -D $MINERVA_PORT minerva"
alias sm="ssh minerva"

alias gamrd="git add-mirror-remote demeter demeter:c/"

export DEFAULT_MIRROR_REMOTE_NAME=demeter
export DEFAULT_MIRROR_REMOTE_HOST=demeter
export DEFAULT_MIRROR_REMOTE_DIR=c

alias gamrm="gamr minerva"
alias gcdm="gcd minerva"

export r1="/datasets/martignetti_ovarian//189/Illumina_DNA/PT189_Right_Ovary_L1/Processed/NGS.2_7_2b.WES/results/PT189_Right_Ovary_L1.PT189_Right_Ovary_L1.clean.dedup.recal.bam"
export r2="/datasets/martignetti_ovarian//189/Illumina_DNA/PT189_Right_Ovary_L2/Processed/NGS.2_7_2b.WES/results/PT189_Right_Ovary_L2.PT189_Right_Ovary_L2.clean.dedup.recal.bam"

export adam_shell_args="--master yarn --deploy-mode client $ne 10 $ec 4 $em 10g $dm 10g $mf=0.05 --conf spark.shuffle.memoryFraction=0.4 --conf spark.akka.timeout=100000 --conf spark.shuffle.service.enabled=true --conf spark.shuffle.manager=SORT $mo=1024 --conf spark.file.transferTo=false"

export d1="demeter-login1.hpc.mssm.edu"
export d2="demeter-login2.hpc.mssm.edu"
